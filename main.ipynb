{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "train = np.load('feats.npy')\n",
    "train_labels = np.load('labels.npy')\n",
    "np.random.seed(42)\n",
    "theta = np.random.rand(15, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def inference(theta, feats):\n",
    "    \"\"\"make a prediction given a vector of features\"\"\"\n",
    "    eY = softmax(np.matmul(theta, feats))\n",
    "    return eY\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatedPredictionMatrix(inference, label, feats):\n",
    "    \"\"\"creates matrix for updating theta based off of gradient of softmax + cross/entropy\"\"\"\n",
    "    inference[int(label)-1] = 1 - inference[int(label)-1]\n",
    "    return -1 * np.matmul(inference.reshape(len(inference), 1), np.column_stack(feats))\n",
    "\n",
    "def printLoss(inference, label):\n",
    "    \"\"\"print cross entropy loss, assuming actual y is one hot encoded\"\"\"\n",
    "    loss = -math.log(inference[int(label)-1])\n",
    "    print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_gradient_descent(theta, labels, train):\n",
    "    N = len(train)\n",
    "    updateM = np.zeros((15, 10000))\n",
    "    print(\"Beginning \", end = '')\n",
    "    y = np.linalg.norm(theta)\n",
    "    printLoss(inference(theta, train[0]), labels[0])\n",
    "    for x in range(len(train)):\n",
    "        expY = inference(theta, train[x])\n",
    "        update = updatedPredictionMatrix(expY, labels[x], train[x])\n",
    "        if ( x % 1000 == 0):\n",
    "            print('Step', str(x)+'/'+str(N)+'  ')\n",
    "        update = np.add(update, updateM)\n",
    "    theta = np.add(theta, update)\n",
    "    print(\"End \", end='')\n",
    "    printLoss(inference(theta, train[0]), labels[0])\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Beginning Loss: 2.701028908100935\n",
      "Step 0/3000  \n",
      "Step 1000/3000  \n",
      "Step 2000/3000  \n",
      "End Loss: 2.7010040846799583\n",
      "epoch 1\n",
      "Beginning Loss: 2.7010040846799583\n",
      "Step 0/3000  \n",
      "Step 1000/3000  \n",
      "Step 2000/3000  \n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(\"epoch\", x)\n",
    "    theta = full_gradient_descent(theta, train_labels, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
