{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "train = np.load('feats.npy')\n",
    "train_labels = np.load('labels.npy')\n",
    "np.random.seed(42)\n",
    "CLASSES = 15\n",
    "DIMENSIONS = 10000\n",
    "LEARNINGRATE = 1000\n",
    "REGULARIZATION = .9\n",
    "theta = 2 * np.random.rand(CLASSES, DIMENSIONS) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"helper functions\"\"\"\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def inference(theta, feats):\n",
    "    \"\"\"make a prediction given a vector of features\"\"\"\n",
    "    eY = softmax(np.matmul(theta, feats))\n",
    "    return eY\n",
    "    \n",
    "def gradient(inference, label, feats):\n",
    "    \"\"\"creates matrix for updating theta based off of gradient of softmax + cross/entropy\"\"\"\n",
    "    inference = -1 * inference\n",
    "    inference[int(label)-1] = 1 + inference[int(label)-1]\n",
    "    update = np.matmul(inference.reshape(len(inference), 1), np.column_stack(feats))\n",
    "    return LEARNINGRATE * update\n",
    "\n",
    "def printLoss(inference, label):\n",
    "    \"\"\"print cross entropy loss, assuming actual y is one hot encoded\"\"\"\n",
    "    loss = -math.log(inference[int(label)-1])\n",
    "    print('Loss:', loss)\n",
    "    \n",
    "def loss(inference, label):\n",
    "    \"\"\"return cross entropy loss, assuming actual y is one hot encoded\"\"\"\n",
    "    try:\n",
    "        ans = -math.log(inference[int(label)-1])\n",
    "        return ans\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def evalLoss(theta, train, label):\n",
    "    avgloss = 0\n",
    "    for x in range(len(train)):\n",
    "        los = loss(softmax(inference(theta, train[x])), label[x])\n",
    "#         pred = softmax(inference(theta, train[x]))\n",
    "        avgloss += los\n",
    "#         if (los > 3):\n",
    "#             print(los)\n",
    "#             qualitative(x, theta, train, label)\n",
    "    return avgloss / len(train)\n",
    "\n",
    "def evalAccuracy(theta, train, label):\n",
    "    correct = 0\n",
    "    for x in range(len(train)):\n",
    "        pred = np.argmax(softmax(inference(theta, train[x])))\n",
    "        if (pred == int(label[x])):\n",
    "            correct += 1\n",
    "    return round(correct / len(train), 4)\n",
    "\n",
    "def evalMetrics(theta, train, labels):\n",
    "    print(\"Matrix Norm:\", np.linalg.norm(theta))\n",
    "    print(\"Loss: \", end='')\n",
    "    print(evalLoss(theta, train, labels))\n",
    "    print('Accuracy: ', str(evalAccuracy(theta, train, labels))+'%')\n",
    "    \n",
    "def qualitative(y, theta, train, labels):\n",
    "    pred = softmax(inference(theta, train[y]))\n",
    "    for i, x in enumerate(pred):\n",
    "        print(i+1, x)\n",
    "    print('prediction:', np.argmax(pred)+1)\n",
    "    print('actual:', int(labels[y]))\n",
    "    print('loss:', loss(pred, labels[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def full_gradient_descent(theta, train, labels):\n",
    "    N = len(train)\n",
    "    updateM = np.zeros((CLASSES, DIMENSIONS))\n",
    "    for x in range(len(train)):\n",
    "        expY = inference(theta, train[x])\n",
    "        update = gradient(expY, labels[x], train[x])\n",
    "        if ( x % 1000 == 0):\n",
    "            print('Step', str(x)+'/'+str(N)+'  ')\n",
    "        update = np.add(update, updateM)\n",
    "        \n",
    "    update = np.add(update, -REGULARIZATION * theta)\n",
    "    theta = np.add(theta, update)\n",
    "    print(\"End \", end='')\n",
    "    evalMetrics(theta, train, labels);\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stochastic_descent(theta, train, labels):\n",
    "    N = len(train)\n",
    "    updateM = np.zeros((CLASSES, DIMENSIONS))\n",
    "    for x in range(int(len(train)/10)):\n",
    "        randSample = np.random.randint(0, train.shape[0])\n",
    "        expY = inference(theta, train[randSample]) \n",
    "        update = gradient(expY, labels[randSample], train[randSample])\n",
    "        update = np.add(update, updateM)\n",
    "    update = np.add(update, -REGULARIZATION * theta)\n",
    "    #regularization to prevent theta from getting too large\n",
    "    theta = np.add(theta, update)\n",
    "    evalMetrics(theta, train, labels)\n",
    "    return theta\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Loss: 2.7080139098107967\n",
      "Beginning epoch 0\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stochastic_descent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-3133f7580dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beginning epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstochastic_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stochastic_descent' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Beginning Loss: \", end = '')\n",
    "print(evalLoss(theta, train, train_labels))\n",
    "for x in range(10):\n",
    "    print(\"Beginning epoch\", x, end = '\\n\\n')\n",
    "    theta = stochastic_descent(theta, train, train_labels)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0821442113914451e-307\n",
      "9.481541138950335e-33\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y = 0\n",
    "for x in inference(theta, train1):\n",
    "    print(x)\n",
    "    y+= x\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2 * np.random.rand(3, 2) - 1\n",
    "train1 = [1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference:\n",
      "[1. 0. 0.]\n",
      "update:\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "inf = inference(theta, train1)\n",
    "feats = train1\n",
    "label = 1\n",
    "print(\"inference:\")\n",
    "print(inf)\n",
    "print(\"update:\")\n",
    "update = gradient(inf, label, feats)\n",
    "for x in update:\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "theta = theta + update\n",
    "for x in inference(theta, train1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -999.49297207,  -9998.66635269],\n",
       "       [  6104.38489339,  61042.2198293 ],\n",
       "       [ -1104.19759098, -11040.06998455]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inference(theta, train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
