{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "def generateVocab():\n",
    "    with open('training.txt', 'r') as data:\n",
    "            lines = data.read().splitlines() \n",
    "    occurences = {}\n",
    "    doccurence = {}\n",
    "    appearances = {}\n",
    "    uniquewords = {}\n",
    "    for x in range(len(lines)):\n",
    "        cI = x\n",
    "        uniquewords[cI] = []\n",
    "        appearances[cI] = {}\n",
    "        currentLine = lines[x].split(' ')\n",
    "        alreadyin = {}\n",
    "        for y in range(1000):\n",
    "            try:\n",
    "                x = alreadyin[currentLine[y]]\n",
    "            except KeyError:\n",
    "                alreadyin[currentLine[y]] = True\n",
    "                try:\n",
    "                    doccurence[currentLine[y]] += 1\n",
    "                    uniquewords[x].append(currentLine[y])\n",
    "                except KeyError:\n",
    "                    doccurence[currentLine[y]] = 1\n",
    "            try:\n",
    "                occurences[currentLine[y]] += 1\n",
    "            except KeyError: \n",
    "                occurences[currentLine[y]] = 1\n",
    "            finally:\n",
    "                try: \n",
    "                    appearances[cI][currentLine[y]] += 1\n",
    "                except KeyError:\n",
    "                    appearances[cI][currentLine[y]] = 1\n",
    "                \n",
    "                    \n",
    "                    \n",
    "            \n",
    "    occurences = {k: v for k, v in sorted(occurences.items(), key=lambda item: item[1], reverse=True)}\n",
    "    with open('unique_words_per_doc.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(uniquewords))\n",
    "    with open('appearances_in_doc.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(appearances))\n",
    "    with open('occurences.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(occurences))\n",
    "    with open('doccurence.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(doccurence))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'main', 'another', 'curious', 'and', 'picturesque', 'series', 'of', 'wall', 'gardens', 'are', 'made', 'by', 'thin', 'streams', 'that', 'slowly', 'from', 'and', 'slip', 'gently', 'over', 'smooth', 'slopes', 'from', 'of', 'sand', 'and', 'mud', 'they', 'carry', 'a', 'pair', 'of', 'sheets', 'of', 'soil', 'an', 'inch', 'or', 'two', 'thick', 'are', 'formed', 'one', 'of', 'them', 'hanging', 'down', 'from', 'the', 'brow', 'of', 'the', 'slope', 'the', 'other', 'leaning', 'up', 'our', 'national', 'from', 'the', 'foot', 'of', 'it', 'like', 'and', 'the', 'soil', 'being', 'held', 'together', 'by', 'the', 'moisture', 'plants', 'growing', 'in', 'it', 'along', 'the', 'rocky', 'parts', 'of', 'the', 'between', 'lake', 'where', 'the', 'streams', 'flow', 'fast', 'over', 'polished', 'granite', 'there', 'are', 'rows', 'of', 'gardens', 'full', 'of', 'and', 'other', 'common', 'plants', 'of', 'the', 'neighborhood', 'nicely', 'arranged', 'like', 'and', 'out', 'in', 'telling', 'on', 'the', 'bare', 'shining', 'rock', 'banks', 'and', 'au', 'the', 'way', 'up', 'the', 'to', 'the', 'summit', 'mountains', 'wherever', 'there', 'is', 'soil', 'of', 'any', 'sort', 'there', 'is', 'no', 'lack', 'of', 'flowers', 'however', 'short', 'the', 'summer', 'may', 'be', 'within', 'eight', 'or', 'ten', 'feet', 'of', 'a', 'snow', 'bank', 'lingering', 'beneath', 'a', 'shadow', 'you', 'may', 'see', 'their', 'in', 'september', 'and', 'hurrying', 'up', 'their', 'brown', 'on', 'ground', 'that', 'has', 'been', 'free', 'from', 'snow', 'only', 'eight', 'or', 'ten', 'days', 'and', 'likely', 'to', 'be', 'covered', 'again', 'within', 'a', 'few', 'weeks', 'the', 'winter', 'in', 'the', 'of', 'these', 'shadow', 'gardens', 'being', 'about', 'eleven', 'months', 'long', 'while', 'spring', 'summer', 'and', 'autumn', 'are', 'hurried', 'and', 'crowded', 'into', 'one', 'month', 'again', 'under', 'favorable', 'conditions', 'gardens', 'three', 'or', 'four', 'thousand', 'feet', 'higher', 'than', 'the', 'last', 'are', 'in', 'then', 'prime', 'in', 'june', 'between', 'the', 'summit', 'peaks', 'at', 'the', 'head', 'of', 'the', 'sur', 'effects', 'are', 'produced', 'where', 'the', 'sunshine', 'falls', 'direct', 'on', 'rocky', 'slopes', 'and', 'among', 'toward', 'the', 'end', 'of', 'august', 'in', 'wild', 'of', 'the', 'park', 'one', 'of', 'these', 'natural', 'on', 'the', 'north', 'shore', 'of', 'a', 'lake', 'feet', 'above', 'the', 'sea', 'i', 'found', 'a', 'luxuriant', 'growth', 'of', 'hairy', 'and', 'the', 'mountain', 'with', 'thousands', 'of', 'purple', 'flowers', 'an', 'inch', 'wide', 'while', 'the', 'opposite', 'shore', 'at', 'a', 'distance', 'of', 'only', 'three', 'hundred', 'yards', 'was', 'bound', 'in', 'heavy', 'snow', 'â', 'on', 'one', 'â', 'de', 'winter', 'on', 'the', 'other', 'and', 'i', 'know', 'a', 'bench', 'garden', 'on', 'the', 'north', 'wall', 'of', 'in', 'which', 'a', 'few', 'flowers', 'are', 'in', 'bloom', 'all', 'winter', 'the', 'massive', 'rocks', 'about', 'it', 'up', 'sunshine', 'enough', 'in', 'summer', 'to', 'melt', 'the', 'snow', 'about', 'as', 'fast', 'as', 'it', 'falls', 'when', 'tired', 'of', 'the', 'confinement', 'of', 'my', 'cabin', 'i', 'used', 'to', 'camp', 'out', 'in', 'it', 'in', 'january', 'and', 'never', 'failed', 'to', 'find', 'flowers', 'and', 'also', 'except', 'during', 'and', 'a', 'few', 'days', 'after', 'from', 'one', 'can', 'easily', 'walk', 'in', 'a', 'day', 'to', 'the', 'top', 'of', 'mount', 'a', 'massive', 'nay', 'mountain', 'tiiat', 'rises', 'in', 'the', 'centre', 'of', 'the', 'park', 'with', 'easy', 'slopes', 'adorned', 'with', 'piles', 'and', 'on', 'the', 'south', 'side', 'rugged', 'with', 'perpetual', 'snow', 'on', 'the', 'north', 'most', 'of', 'the', 'broad', 'summit', 'is', 'comparatively', 'level', 'and', 'smooth', 'and', 'covered', 'with', 'of', 'tour', 'etc', 'out', 'and', 'strewn', 'loosely', 'as', 'if', 'sown', 'their', 'radiance', 'so', 'in', 'some', 'places', 'as', 'to', 'fairly', 'hide', 'the', 'multitude', 'of', 'c', 'our', 'national', 'small', 'flowers', 'that', 'grow', 'among', 'them', 'of', 'keen', 'lance', 'rays', 'infinitely', 'fine', 'white', 'or', 'colored', 'making', 'an', 'almost', 'continuous', 'glow', 'over', 'all', 'the', 'ground', 'with', 'here', 'and', 'there', 'throbbing', 'lilies', 'of', 'on', 'the', 'larger', 'gems', 'at', 'first', 'sight', 'only', 'these', 'crystal', 'are', 'noticed', 'but', 'looking', 'closely', 'you', 'discover', 'minute', 'g', 'etc', 'in', 'thousands', 'showing', 'more', 'than', 'leaves', 'and', 'larger', 'plants', 'in', 'hollows', 'and', 'on', 'the', 'borders', 'of', 'â', 'mountain', 'fringed', 'with', 'you', 'wander', 'about', 'from', 's', 'to', 's', 'enchanted', 'â', 'if', 'talking', 'b', 'thâ', 'the', 'brightest', 'gems', 'each', 'and', 'all', 'apparently', 'doing', 'their', 'best', 'with', 'eager', 'enthusiasm', 'as', 'if', 'everything', 'depended', 'on', 'faithful', 'shining', 'and', 'considering', 'the', 'flowers', 'in', 'the', 'glorious', 'light', 'many', 'of', 'them', 'looking', 'like', 'of', 'small', 'and', 'that', 'were', 'resting', 'after', 'long', 'dances', 'in', 'the', 'now', 'your', 'attention', 'is', 'called', 'to', 'colonies', 'of', 'and', 'the', 'in', 'front', 'of', 'their', 'glittering', 'like', 'heaps', 'of', 'â', 'romantic', 'ground', 'to', 'live', 'in', 'or', 'die', 'in', 'now', 'you', 'look', 'abroad', 'over', 'the', 'vast', 'round', 'landscape', 'bounded', 'by', 'the', 'down', 'sky', 'nearly', 'all', 'the', 'park', 'in', 'it', 'displayed', 'like', 'a', 'map', 'â', 'forests', 'meadows', 'lakes', 'rock', 'waves', 'and', 'snowy', 'mountains', 'northward', 'lies', 'the', 'basin', 'of', 'creek', 'paved', 'with', 'bright', 'and', 'lakes', 'like', 'larger', 'wild', 'gardens', 'of', 'the', 'park', 'eastward', 'the', 'region', 'and', 'the', 'summit', 'peaks', 'in', 'glorious', 'array', 'southward', 'and', 'westward', 'the', 'boundless', 'forests', 'on', 'no', 'other', 'mountain', 'that', 'i', 'know', 'of', 'are', 'you', 'more', 'likely', 'to', 'linger', 'it', 'is', 'a', 'magnificent', 'camp', 'ground', 'of', 'dwarf', 'pine', 'furnish', 'roots', 'and', 'branches', 'for', 'fuel', 'and', 'the', 'pure', 'water', 'around', 'your', 'camp', 'fire', 'the', 'flowers', 'seem', 'to', 'be', 'looking', 'eagerly', 'at', 'the', 'and', 'the', 'shine', 'making', 'company', 'as', 'you', 'ke', 'at', 'rest', 'in', 'the', 'y', 'ry', 'heart', 'of', 'the', 'vast', 'serene', 'majestic', 'night', 'the', 'finest', 'of', 'the', 'meadow', 'gardens', 'lie', 'at', 'an', 'elevation', 'of', 'about', 'nine', 'thousand', 'feet', 'in', 'the', 'upper', 'pine', 'forests', 'like', 'lakes', 'of', 'light', 'they', 'are', 'smooth', 'and', 'level', 'a', 'mile', 'or', 'two', 'long', 'and', 'the', 'rich', 'well', 'drained', 'ground', 'is', 'completely', 'covered', 'with', 'a', 'soft', 'sod', 'with', 'flowers', 'not', 'one', 'of', 'which', 'is', 'in', 'the', 'least', 'or', 'coarse', 'in', 'some', 'places', 'the', 'sod', 'is', 'so', 'crowded', 'with', 'flowers', 'that', 'the', 'are', 'scarce', 'noticed', 'in', 'others', 'they', 'are', 'rather', 'scattered', 'while', 'every', 'leaf', 'and', 'flower', 'seems', 'to', 'have', 'its', 'winged', 'representative', 'in', 'the', 'of', 'happy', 'flower', 'like', 'insects', 'that', 'the', 'air', 'above', 'them', 'with', 'the', 'winter', 'wings', 'and', 'are', 'folded', 'and', 'for', 'more', 'than', 'half', 'the', 'year', 'the', 'meadows', 'are', 'snow', 'ten', 'or', 'fifteen', 'deep', 'in', 'june', 'they', 'begin', 'to', 'out', 'small', 'patches', 'of', 'our', 'national', 'the', 'dead', 'sod', 'appear', 'gradually', 'increasing', 'in', 'size', 'until', 'they', 'are', 'free', 'and', 'warm', 'a', 'fain', 'face', 'to', 'face', 'ma', 'the', 'sky', 'of', 'push', 'through', 'the', 'steaming', 'mould', 'sing', 'soon', 'joined', 'by', 'the', 'birds', 'and', 'the', 'merry', 'insects', 'come', 'back', 'as', 'if', 'suddenly', 'raised', 'from', 'the', 'dead', 'soon', 't', 'e', 'ground', 'is', 'een', 'th', 'and', 'and', 'dotted', 'with', 'small', 'making', 'the', 'first', 'crop', 'of', 'the', 'season', 'then', 'the', 'grass', 'leaves', 'a', 'new', 'sod', 'and', 'the', 'exceedingly', 'slender', 'rise', 'above', 'it', 'like', 'a', 'purple', 'mist', ',13']\n"
     ]
    }
   ],
   "source": [
    "generateVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bag_of_Word_Feature_Extractor(string):\n",
    "    occurences = generateVocab()\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    for x in range(len(string)):\n",
    "        ans[bag[string[x]]] += 1\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_Feature_Extractor(string):\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    uniqueWords = []\n",
    "    for z in range(len(string)):\n",
    "        if string[z] not in uniqueWords:\n",
    "            uniqueWords.append(string[z])\n",
    "    with open('training.txt', 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    totalDocs = len(lines);\n",
    "    for x in range(len(uniqueWords)):\n",
    "        cAppearances = 0\n",
    "        for y in range(len(string)):\n",
    "            if string[y] == uniqueWords[x]:\n",
    "                cAppearances += 1\n",
    "        #calculate appearances in document\n",
    "        dApperances = 0;\n",
    "        for y in range(totalDocs):\n",
    "            currentLine = lines[y].split(' ')\n",
    "            if uniqueWords[x] in currentLine:\n",
    "                dApperances += 1\n",
    "        #calculate appearances in all documents\n",
    "        print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "        if (dApperances == 0):\n",
    "            continue\n",
    "        else:\n",
    "            ans[bag[uniqueWords[x]]] = (cAppearances/len(string)) * math.log(totalDocs/dApperances)\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureVectors():\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    result = ()\n",
    "    kL = len(occurences.keys())\n",
    "    with open('training.txt', 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    with open('doccurence.txt', 'r') as fin:\n",
    "        docAp = json.loads(fin.read())\n",
    "    with open('unique_words_per_doc.txt', 'r') as fin:\n",
    "        allUniqueWords = json.loads(fin.read())\n",
    "    with open('appearances_in_doc.txt', 'r') as fin:\n",
    "        cap = json.loads(fin.read())\n",
    "    totalDocs = len(lines);\n",
    "    for i, y in enumerate(lines): \n",
    "        string = y.split(' ')\n",
    "        ans = np.zeros(kL)\n",
    "        uniqueWords = allUniqueWords[i]\n",
    "        for x in range(len(uniqueWords)):\n",
    "            cAppearances = cap[uniqueWords[x]]\n",
    "            #calculate appearances in document\n",
    "            dApperances = docAp[uniqueWords[x]]\n",
    "            #print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "            if (dApperances == 0):\n",
    "                continue\n",
    "            else:\n",
    "                ans[bag[uniqueWords[x]]] = (cAppearances/len(string)) * math.log(totalDocs/dApperances)\n",
    "        result = result + (ans,)\n",
    "        print(result)\n",
    "    ans = np.column_stack(result)\n",
    "    np.tofile(ans)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-56d3d686568e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerateFeatureVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-522a0f6662e6>\u001b[0m in \u001b[0;36mgenerateFeatureVectors\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muniqueWords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcAppearances\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalDocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdApperances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generateFeatureVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2995\n"
     ]
    }
   ],
   "source": [
    "with open('doccurence.txt', 'r') as fin:\n",
    "    docAp = json.loads(fin.read())\n",
    "print(docAp['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
