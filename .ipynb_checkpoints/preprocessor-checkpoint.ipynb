{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "def generateVocab():\n",
    "    with open('training.txt', 'r') as data:\n",
    "            lines = data.read().splitlines() \n",
    "    occurences = {}\n",
    "    doccurence = {}\n",
    "    for x in range(len(lines)):\n",
    "        currentLine = lines[x].split(' ')\n",
    "        alreadyin = {}\n",
    "        for y in range(1000):\n",
    "            try:\n",
    "                x = alreadyin[currentLine[y]]\n",
    "            except KeyError:\n",
    "                alreadyin[currentLine[y]] = True\n",
    "                try:\n",
    "                    doccurence[currentLine[y]] += 1\n",
    "                except KeyError:\n",
    "                    doccurence[currentLine[y]] = 1\n",
    "            try:\n",
    "                occurences[currentLine[y]] += 1\n",
    "            except KeyError: \n",
    "                occurences[currentLine[y]] = 1\n",
    "            \n",
    "    occurences = {k: v for k, v in sorted(occurences.items(), key=lambda item: item[1], reverse=True)}\n",
    "    with open()\n",
    "    with open('unique_words_per_doc.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(uniquewords))\n",
    "    with open('occurences.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(occurences))\n",
    "    with open('doccurence.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(doccurence))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bag_of_Word_Feature_Extractor(string):\n",
    "    occurences = generateVocab()\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    for x in range(len(string)):\n",
    "        ans[bag[string[x]]] += 1\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_Feature_Extractor(string):\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    uniqueWords = []\n",
    "    for z in range(len(string)):\n",
    "        if string[z] not in uniqueWords:\n",
    "            uniqueWords.append(string[z])\n",
    "    with open('training.txt', 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    totalDocs = len(lines);\n",
    "    for x in range(len(uniqueWords)):\n",
    "        cAppearances = 0\n",
    "        for y in range(len(string)):\n",
    "            if string[y] == uniqueWords[x]:\n",
    "                cAppearances += 1\n",
    "        #calculate appearances in document\n",
    "        dApperances = 0;\n",
    "        for y in range(totalDocs):\n",
    "            currentLine = lines[y].split(' ')\n",
    "            if uniqueWords[x] in currentLine:\n",
    "                dApperances += 1\n",
    "        #calculate appearances in all documents\n",
    "        print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "        if (dApperances == 0):\n",
    "            continue\n",
    "        else:\n",
    "            ans[bag[uniqueWords[x]]] = (cAppearances/len(string)) * math.log(totalDocs/dApperances)\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureVectors():\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    result = ()\n",
    "    kL = len(occurences.keys())\n",
    "    with open('training.txt', 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    with open('doccurence.txt', 'r') as fin:\n",
    "        docAp = json.loads(fin.read())\n",
    "    with open('unique_words_per_doc.txt', 'r') as fin:\n",
    "        uniqueWords = json.loads(fin.read())\n",
    "    with open()\n",
    "    totalDocs = len(lines);\n",
    "    for y, x in enumerate(lines): \n",
    "        string = x.split(' ')\n",
    "        ans = np.zeros(kL)\n",
    "        uniqueWords = uniqueWords[y]\n",
    "        for x in range(len(uniqueWords)):\n",
    "            cAppearances = 0\n",
    "            for y in range(len(string)):\n",
    "                if string[y] == uniqueWords[x]:\n",
    "                    cAppearances += 1\n",
    "            #calculate appearances in document\n",
    "            dApperances = docAp[uniqueWords[x]]\n",
    "    #             print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "            if (dApperances == 0):\n",
    "                continue\n",
    "            else:\n",
    "                ans[bag[uniqueWords[x]]] = (cAppearances/len(string)) * math.log(totalDocs/dApperances)\n",
    "        result = result + (ans,)\n",
    "    ans = np.column_stack(result)\n",
    "    np.tofile(ans)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-56d3d686568e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerateFeatureVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-522a0f6662e6>\u001b[0m in \u001b[0;36mgenerateFeatureVectors\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muniqueWords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcAppearances\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalDocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdApperances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generateFeatureVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2995\n"
     ]
    }
   ],
   "source": [
    "with open('doccurence.txt', 'r') as fin:\n",
    "    docAp = json.loads(fin.read())\n",
    "print(docAp['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
