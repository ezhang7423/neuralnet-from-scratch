{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "def generateVocab():\n",
    "    with open('training.txt', 'r') as data:\n",
    "            lines = data.read().splitlines() \n",
    "    occurences = {}\n",
    "    doccurence = {}\n",
    "    appearances = {}\n",
    "    uniquewords = {}\n",
    "    for x in range(len(lines)):\n",
    "        uniquewords[x] = []\n",
    "        appearances[x] = {}\n",
    "        currentLine = lines[x].split(' ')\n",
    "        alreadyin = {}\n",
    "        for y in range(1000):\n",
    "            if currentLine[y].isalnum():\n",
    "                try:\n",
    "                    x = alreadyin[currentLine[y]]\n",
    "                except KeyError:\n",
    "                    alreadyin[currentLine[y]] = True\n",
    "                    try:\n",
    "                        doccurence[currentLine[y]] += 1\n",
    "                        uniquewords[x].append(currentLine[y])\n",
    "                    except KeyError:\n",
    "                        doccurence[currentLine[y]] = 1\n",
    "                try:\n",
    "                    occurences[currentLine[y]] += 1\n",
    "                except KeyError: \n",
    "                    occurences[currentLine[y]] = 1\n",
    "                finally:\n",
    "                    try: \n",
    "                        appearances[x][currentLine[y]] += 1\n",
    "                    except KeyError:\n",
    "                        print(x)\n",
    "#                         appearances[x][currentLine[y]] = 1\n",
    "                        print(appearances[x])\n",
    "    occurences = {k: v for k, v in sorted(occurences.items(), key=lambda item: item[1], reverse=True)}\n",
    "    with open('unique_words_per_doc.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(uniquewords))\n",
    "    with open('appearances_in_doc.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(appearances))\n",
    "    with open('occurences.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(occurences))\n",
    "    with open('doccurence.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(doccurence))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "0\n",
      "{}\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7b88d91112d4>\u001b[0m in \u001b[0;36mgenerateVocab\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                         \u001b[0mappearances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrentLine\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: True",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-486a799607b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerateVocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-7b88d91112d4>\u001b[0m in \u001b[0;36mgenerateVocab\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#                         appearances[x][currentLine[y]] = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappearances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0moccurences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moccurences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unique_words_per_doc.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: True"
     ]
    }
   ],
   "source": [
    "generateVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bag_of_Word_Feature_Extractor(string):\n",
    "    occurences = generateVocab()\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    for x in range(len(string)):\n",
    "        ans[bag[string[x]]] += 1\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_Feature_Extractor(string):\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    uniqueWords = []\n",
    "    for z in range(len(string)):\n",
    "        if string[z] not in uniqueWords:\n",
    "            uniqueWords.append(string[z])\n",
    "    with open('training.txt', 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    totalDocs = len(lines);\n",
    "    for x in range(len(uniqueWords)):\n",
    "        cAppearances = 0\n",
    "        for y in range(len(string)):\n",
    "            if string[y] == uniqueWords[x]:\n",
    "                cAppearances += 1\n",
    "        #calculate appearances in document\n",
    "        dApperances = 0;\n",
    "        for y in range(totalDocs):\n",
    "            currentLine = lines[y].split(' ')\n",
    "            if uniqueWords[x] in currentLine:\n",
    "                dApperances += 1\n",
    "        #calculate appearances in all documents\n",
    "        print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "        if (dApperances == 0):\n",
    "            continue\n",
    "        else:\n",
    "            ans[bag[uniqueWords[x]]] = (cAppearances/len(string)) * math.log(totalDocs/dApperances)\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureVectors():\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    result = ()\n",
    "    kL = len(occurences.keys())\n",
    "    with open('training.txt', 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    with open('doccurence.txt', 'r') as fin:\n",
    "        docAp = json.loads(fin.read())\n",
    "    with open('unique_words_per_doc.txt', 'r') as fin:\n",
    "        allUniqueWords = json.loads(fin.read())\n",
    "    with open('appearances_in_doc.txt', 'r') as fin:\n",
    "        cap = json.loads(fin.read())\n",
    "    totalDocs = len(lines);\n",
    "    for i, y in enumerate(lines): \n",
    "        string = y.split(' ')\n",
    "        ans = np.zeros(kL)\n",
    "        uniqueWords = allUniqueWords[i]\n",
    "        for x in range(len(uniqueWords)):\n",
    "            cAppearances = cap[uniqueWords[x]]\n",
    "            #calculate appearances in document\n",
    "            dApperances = docAp[uniqueWords[x]]\n",
    "            #print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "            ans[bag[uniqueWords[x]]] = (cAppearances/len(string)) * math.log(totalDocs/dApperances)\n",
    "        result = result + (ans,)\n",
    "        print(result)\n",
    "    ans = np.column_stack(result)\n",
    "    np.tofile(ans)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-56d3d686568e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerateFeatureVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-522a0f6662e6>\u001b[0m in \u001b[0;36mgenerateFeatureVectors\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muniqueWords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcAppearances\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotalDocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdApperances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generateFeatureVectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2995\n"
     ]
    }
   ],
   "source": [
    "with open('doccurence.txt', 'r') as fin:\n",
    "    docAp = json.loads(fin.read())\n",
    "print(docAp['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
