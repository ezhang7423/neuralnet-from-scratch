{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "def generateVocab(filename):\n",
    "    with open('data/'+filename, 'r') as data:\n",
    "            lines = data.read().splitlines() \n",
    "    occurences = {}\n",
    "    doccurence = {}\n",
    "    appearances = {}\n",
    "    uniquewords = {}\n",
    "    for x in range(len(lines)):\n",
    "        appearances[x] = {}\n",
    "        currentLine = lines[x].split(' ')\n",
    "        uniquewords[x] = list(filter(lambda x: x.isalnum(), list(set(currentLine))))\n",
    "        alreadyin = {}\n",
    "        for y in range(1000):\n",
    "            if currentLine[y].isalnum():\n",
    "                try:\n",
    "                    z = alreadyin[currentLine[y]]\n",
    "                except KeyError:\n",
    "                    alreadyin[currentLine[y]] = True\n",
    "                    try:\n",
    "                        doccurence[currentLine[y]] += 1\n",
    "                    except KeyError:\n",
    "                        doccurence[currentLine[y]] = 1\n",
    "                        \n",
    "                try:\n",
    "                    occurences[currentLine[y]] += 1\n",
    "                except KeyError: \n",
    "                    occurences[currentLine[y]] = 1\n",
    "                finally:\n",
    "                    try: \n",
    "                        appearances[x][currentLine[y]] += 1\n",
    "                    except KeyError:\n",
    "                        appearances[x][currentLine[y]] = 1\n",
    "    occurences = {k: v for k, v in sorted(occurences.items(), key=lambda item: item[1], reverse=True)}\n",
    "    with open(filename+'unique_words_per_doc.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(uniquewords))\n",
    "    with open(filename+'appearances_in_doc.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(appearances))\n",
    "    with open(filename+'occurences.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(occurences))\n",
    "    with open(filename+'doccurence.txt', 'w') as fout:\n",
    "        fout.write(json.dumps(doccurence))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateVocab('training')\n",
    "generateVocab('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bag_of_Word_Feature_Extractor(string):\n",
    "    occurences = generateVocab()\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    for x in range(len(string)):\n",
    "        ans[bag[string[x]]] += 1\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF_Feature_Extractor(string):\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    #bag maps word to index\n",
    "    string = string.split(' ')\n",
    "    ans = np.zeros(len(occurences.keys()))\n",
    "    uniqueWords = []\n",
    "    for z in range(len(string)):\n",
    "        if string[z] not in uniqueWords:\n",
    "            uniqueWords.append(string[z])\n",
    "    with open('data/training', 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    totalDocs = len(lines);\n",
    "    for x in range(len(uniqueWords)):\n",
    "        cAppearances = 0\n",
    "        for y in range(len(string)):\n",
    "            if string[y] == uniqueWords[x]:\n",
    "                cAppearances += 1\n",
    "        #calculate appearances in document\n",
    "        dApperances = 0;\n",
    "        for y in range(totalDocs):\n",
    "            currentLine = lines[y].split(' ')\n",
    "            if uniqueWords[x] in currentLine:\n",
    "                dApperances += 1\n",
    "        #calculate appearances in all documents\n",
    "        print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "        if (dApperances == 0):\n",
    "            continue\n",
    "        else:\n",
    "            ans[bag[uniqueWords[x]]] = (cAppearances/len(string)) * math.log(totalDocs/dApperances)\n",
    "    return ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureVectors(filename):\n",
    "    with open('occurences.txt', 'r') as fin:\n",
    "        occurences = json.loads(fin.read())\n",
    "    bag = {} \n",
    "    counter = 0\n",
    "    for key in occurences:\n",
    "        bag[key] = counter\n",
    "        counter += 1\n",
    "    result = ()\n",
    "    kL = len(occurences.keys())\n",
    "    with open('data/'+filename, 'r') as data:\n",
    "        lines = data.read().splitlines() \n",
    "    with open(filename+'doccurence.txt', 'r') as fin:\n",
    "        docAp = json.loads(fin.read())\n",
    "    with open(filename+'unique_words_per_doc.txt', 'r') as fin:\n",
    "        allUniqueWords = json.loads(fin.read())\n",
    "    with open(filename+'appearances_in_doc.txt', 'r') as fin:\n",
    "        cap = json.loads(fin.read())\n",
    "    totalDocs = len(lines);\n",
    "    for i, y in enumerate(lines): \n",
    "        string = y.split(' ')\n",
    "        ans = np.zeros(kL)\n",
    "        uniqueWords = allUniqueWords[str(i)]\n",
    "        for x in range(len(uniqueWords)):\n",
    "            cAppearances = cap[str(i)][uniqueWords[x]]\n",
    "            #calculate appearances in document\n",
    "            dApperances = docAp[uniqueWords[x]]\n",
    "            #print(uniqueWords[x],  (cAppearances/len(string)) * math.log(totalDocs/dApperances))\n",
    "            ans[bag[uniqueWords[x]]] = (cAppearances/(len(string) - 1 )) * math.log(totalDocs/dApperances)\n",
    "        result = result + (ans,)\n",
    "    ans = np.row_stack(result)\n",
    "    np.save(filename+'.npy', ans)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateFeatureVectors('training')\n",
    "generateFeatureVectors('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLabels(filename):\n",
    "    with open('data/'+filename, 'r') as fin:\n",
    "        data = fin.read().splitlines()\n",
    "    ans = np.empty(len(data),)\n",
    "    \n",
    "    for x in range(len(data)):\n",
    "        ln = data[x].split(' ')\n",
    "        ans[x] = int(ln[-1][1:len(ln)+1])\n",
    "    np.save(filename+'-labels.npy', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractLabels('training')\n",
    "extractLabels('testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
